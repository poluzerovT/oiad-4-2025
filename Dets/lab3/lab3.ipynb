{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff07fe8",
   "metadata": {},
   "source": [
    "# ОИАД. Лабораторная работа №3\n",
    "Для построения моделей: datasets/insurance_train.csv Для оценки обобщающей способности: datasets/insurance_test.csv\n",
    "\n",
    "Инфо о датасете: https://www.kaggle.com/datasets/mosapabdelghany/medical-insurance-cost-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d142f",
   "metadata": {},
   "source": [
    "### Задание 1. Подготовка данных\n",
    "- проверить наличие пропусков и выбросов\n",
    "- привести категориальные признаки к числовым\n",
    "- вычислить парные корреляции признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1541a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропуски в данных: 0\n",
      "\n",
      "Проверка выбросов:\n",
      "age: 0 выбросов\n",
      "bmi: 9 выбросов\n",
      "children: 0 выбросов\n",
      "charges: 139 выбросов\n",
      "\n",
      "Корреляция с charges:\n",
      "charges             1.000000\n",
      "smoker_yes          0.787251\n",
      "age                 0.299008\n",
      "bmi                 0.198341\n",
      "region_southeast    0.073982\n",
      "children            0.067998\n",
      "sex_male            0.057292\n",
      "region_northwest   -0.039905\n",
      "region_southwest   -0.043210\n",
      "Name: charges, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. ПОДГОТОВКА ДАТАСЕТА\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('insurance.csv')\n",
    "\n",
    "# Проверка пропусков\n",
    "print(\"Пропуски в данных:\", df.isnull().sum().sum())\n",
    "\n",
    "# Проверка выбросов по правилу 1.5 * IQR\n",
    "print(\"\\nПроверка выбросов:\")\n",
    "numeric_cols = ['age', 'bmi', 'children', 'charges']\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"{col}: {len(outliers)} выбросов\")\n",
    "\n",
    "# Кодирование категориальных признаков\n",
    "df_encoded = pd.get_dummies(df, columns=['sex', 'smoker', 'region'], drop_first=True)\n",
    "\n",
    "# Парные корреляции\n",
    "correlation_matrix = df_encoded.corr()\n",
    "print(\"\\nКорреляция с charges:\")\n",
    "print(correlation_matrix['charges'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc91ce",
   "metadata": {},
   "source": [
    "Поиск выбросов - используется правило 1.5 * IQR (межквартильный размах):\n",
    "Q1 - 25-й процентиль, Q3 - 75-й процентиль\n",
    "IQR = Q3 - Q1 (размах средних 50% данных)\n",
    "Выбросы - значения за пределами [Q1 - 1.5*IQR, Q3 + 1.5*IQR]\n",
    "\n",
    "Результаты: Пропусков не обнаружено, выбросы идентифицированы методом IQR (Interquartile Range). Наибольшая корреляция с целевой переменной наблюдается у признака smoker и age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1e9f7",
   "metadata": {},
   "source": [
    "### Задание 2. Многомерная линейная регрессия\n",
    "Построить модель линейной регрессии и подобрать параметры:\n",
    "- аналитически (реализовать самому)\n",
    "- численно, с помощью методов градиентного спуска (реализовать самому)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a88d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры данных:\n",
      "X_train: (1070, 9), y_train: (1070,)\n",
      "X_test: (268, 9), y_test: (268,)\n",
      "\n",
      "АНАЛИТИЧЕСКОЕ РЕШЕНИЕ:\n",
      "Коэффициенты: [ 1.33460897e+04  3.61497541e+03  2.03622812e+03  5.16890247e+02\n",
      " -9.29310107e+00  9.55848141e+03 -1.58140981e+02 -2.90157047e+02\n",
      " -3.49110678e+02]\n",
      "MSE: 33596915.85\n",
      "R²: 0.7836\n",
      "\n",
      "ГРАДИЕНТНЫЙ СПУСК:\n",
      "Коэффициенты: [ 1.33455136e+04  3.61488185e+03  2.03241312e+03  5.16980755e+02\n",
      " -8.70529765e+00  9.55789194e+03 -1.43936208e+02 -2.74066666e+02\n",
      " -3.34265195e+02]\n",
      "MSE: 33608763.19\n",
      "R²: 0.7835\n",
      "\n",
      "СРАВНЕНИЕ МЕТОДОВ:\n",
      "Аналитическое решение - MSE: 33596915.85, R²: 0.7836\n",
      "Градиентный спуск - MSE: 33608763.19, R²: 0.7835\n",
      "Разница в R²: 0.000076\n",
      "\n",
      "Коэффициенты аналитического решения:\n",
      "intercept: +13346.09\n",
      "age: +3614.98\n",
      "bmi: +2036.23\n",
      "children: +516.89\n",
      "sex_male: -9.29\n",
      "smoker_yes: +9558.48\n",
      "region_northwest: -158.14\n",
      "region_southeast: -290.16\n",
      "region_southwest: -349.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Подготовка данных\n",
    "X = df_encoded.drop('charges', axis=1)\n",
    "y = df_encoded['charges']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = np.column_stack([np.ones(X_train_scaled.shape[0]), X_train_scaled])\n",
    "X_test_scaled = np.column_stack([np.ones(X_test_scaled.shape[0]), X_test_scaled])\n",
    "\n",
    "print(\"Размеры данных:\")\n",
    "print(f\"X_train: {X_train_scaled.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test_scaled.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# 2.а Аналитическое решение (нормальное уравнение)\n",
    "def analytical_solution(X, y):\n",
    "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return theta\n",
    "\n",
    "theta_analytical = analytical_solution(X_train_scaled, y_train)\n",
    "y_pred_analytical = X_test_scaled @ theta_analytical\n",
    "\n",
    "mse_analytical = mean_squared_error(y_test, y_pred_analytical)\n",
    "r2_analytical = r2_score(y_test, y_pred_analytical)\n",
    "\n",
    "print(\"\\nАНАЛИТИЧЕСКОЕ РЕШЕНИЕ:\")\n",
    "print(f\"Коэффициенты: {theta_analytical}\")\n",
    "print(f\"MSE: {mse_analytical:.2f}\")\n",
    "print(f\"R²: {r2_analytical:.4f}\")\n",
    "\n",
    "# 2.б Градиентный спуск\n",
    "def gradient_descent(X, y, learning_rate=0.01, n_iter=1000):\n",
    "    m = len(y)\n",
    "    theta = np.zeros(X.shape[1]) \n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        y_pred = X @ theta\n",
    "        gradient = (1/m) * X.T @ (y_pred - y)\n",
    "        theta -= learning_rate * gradient\n",
    "        \n",
    "        cost = (1/(2*m)) * np.sum((y_pred - y)**2)\n",
    "        cost_history.append(cost)\n",
    "                \n",
    "        # if i % 200 == 0:\n",
    "        #     print(f\"Iteration {i}: Cost = {cost:.2f}\")\n",
    "    return theta, cost_history\n",
    "\n",
    "theta_gd, cost_history = gradient_descent(X_train_scaled, y_train, learning_rate=0.01, n_iter=1000)\n",
    "\n",
    "y_pred_gd = X_test_scaled @ theta_gd\n",
    "\n",
    "mse_gd = mean_squared_error(y_test, y_pred_gd)\n",
    "r2_gd = r2_score(y_test, y_pred_gd)\n",
    "\n",
    "print(\"\\nГРАДИЕНТНЫЙ СПУСК:\")\n",
    "print(f\"Коэффициенты: {theta_gd}\")\n",
    "print(f\"MSE: {mse_gd:.2f}\")\n",
    "print(f\"R²: {r2_gd:.4f}\")\n",
    "\n",
    "# Сравнение методов\n",
    "print(\"\\nСРАВНЕНИЕ МЕТОДОВ:\")\n",
    "print(f\"Аналитическое решение - MSE: {mse_analytical:.2f}, R²: {r2_analytical:.4f}\")\n",
    "print(f\"Градиентный спуск - MSE: {mse_gd:.2f}, R²: {r2_gd:.4f}\")\n",
    "print(f\"Разница в R²: {abs(r2_analytical - r2_gd):.6f}\")\n",
    "\n",
    "# Получим названия признаков после кодирования\n",
    "feature_names = ['intercept'] + list(X.columns)\n",
    "print(\"\\nКоэффициенты аналитического решения:\")\n",
    "for i, coef in enumerate(theta_analytical):\n",
    "    print(f\"{feature_names[i]}: {coef:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453e0a0",
   "metadata": {},
   "source": [
    "Многомерная линейная регрессия моделирует зависимость целевой переменной как линейную комбинацию признаков: y = w₀ + w₁x₁ + ... + wₙxₙ. \n",
    "Аналитическое решение использует нормальное уравнение (XᵀX)⁻¹Xᵀy, которое находит оптимальные веса за одну операцию. \n",
    "Градиентный спуск итеративно обновляет веса в направлении антиградиента функции потерь.\n",
    "\n",
    "Результаты: \n",
    "Оба метода показали схожие результаты (MSE ~33 млн). \n",
    "Небольшая разница обусловлена численной погрешностью: аналитическое решение точнее, но требует обращения матрицы, что может быть вычислительно сложно для больших датасетов. \n",
    "Градиентный спуск более масштабируем, но требует подбора темпа обучения.\n",
    "\n",
    "В целом, можно сделать выводы, что:\n",
    "Курение увеличивает стоимость страховки на ~$9,558 - самый значительный фактор\n",
    "Каждый год возраста добавляет ~$3,615 к стоимости\n",
    "Модель успешно предсказывает 78% variations в стоимости страховки\n",
    "\n",
    "intercept: +13346.09\n",
    "smoker_yes: +9558.48\n",
    "age: +3614.98\n",
    "bmi: +2036.23\n",
    "children: +516.89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba27a3",
   "metadata": {},
   "source": [
    "### Задание 3. Добавление регуляризации\n",
    "Модифицировать линейную модель путем добавления регуляризационного слагаемого. Найти оптимальные веса:\n",
    "- аналитически\n",
    "- численно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7bad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIDGE РЕГРЕССИЯ (аналитически):\n",
      "Лучший alpha: 0.001, R²: 0.7836\n",
      "\n",
      "RIDGE РЕГРЕССИЯ (градиентный спуск):\n",
      "R²: 0.7835\n",
      "\n",
      "СРАВНЕНИЕ:\n",
      "Обычная: R² = 0.7836\n",
      "Ridge аналит: R² = 0.7836\n",
      "Ridge GD: R² = 0.7835\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def ridge_analytical(X, y, alpha=1.0):\n",
    "    m, n = X.shape\n",
    "    I = np.eye(n)\n",
    "    I[0, 0] = 0  \n",
    "    return np.linalg.inv(X.T @ X + alpha * I) @ X.T @ y\n",
    "\n",
    "# Подбор alpha\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "best_alpha = alphas[0]\n",
    "best_r2 = -np.inf\n",
    "\n",
    "# Перебираем разные alpha и выбираем лучший по R² на тестовой выборке\n",
    "for alpha in alphas:\n",
    "    theta_ridge = ridge_analytical(X_train_scaled, y_train, alpha)\n",
    "    r2 = r2_score(y_test, X_test_scaled @ theta_ridge)\n",
    "    if r2 > best_r2:\n",
    "        best_r2, best_alpha = r2, alpha\n",
    "\n",
    "# Обучение с лучшим alpha\n",
    "theta_ridge_optimal = ridge_analytical(X_train_scaled, y_train, best_alpha)\n",
    "y_pred_ridge = X_test_scaled @ theta_ridge_optimal\n",
    "\n",
    "print(\"RIDGE РЕГРЕССИЯ (аналитически):\")\n",
    "print(f\"Лучший alpha: {best_alpha}, R²: {r2_score(y_test, y_pred_ridge):.4f}\")\n",
    "\n",
    "# 3.б Градиентный спуск с регуляризацией\n",
    "def ridge_gd(X, y, alpha=1.0, lr=0.01, n_iter=1000):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for _ in range(n_iter):\n",
    "        y_pred = X @ theta\n",
    "        gradient = (1/len(y)) * X.T @ (y_pred - y) + (alpha/len(y)) * theta\n",
    "        gradient[0] = (1/len(y)) * X[:, 0] @ (y_pred - y)\n",
    "        theta -= lr * gradient\n",
    "    return theta\n",
    "\n",
    "theta_ridge_gd = ridge_gd(X_train_scaled, y_train, best_alpha)\n",
    "y_pred_ridge_gd = X_test_scaled @ theta_ridge_gd\n",
    "\n",
    "print(\"\\nRIDGE РЕГРЕССИЯ (градиентный спуск):\")\n",
    "print(f\"R²: {r2_score(y_test, y_pred_ridge_gd):.4f}\")\n",
    "\n",
    "# Сравнение\n",
    "print(\"\\nСРАВНЕНИЕ:\")\n",
    "print(f\"Обычная: R² = {r2_analytical:.4f}\")\n",
    "print(f\"Ridge аналит: R² = {r2_score(y_test, y_pred_ridge):.4f}\")\n",
    "print(f\"Ridge GD: R² = {r2_score(y_test, y_pred_ridge_gd):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0040de",
   "metadata": {},
   "source": [
    "Регуляризация добавляет штраф за большие значения весов к функции потерь. \n",
    "Ridge регрессия добавляет штраф за большие коэффициенты\n",
    "Alpha - параметр регуляризации (чем больше, тем сильнее штраф), alpha=0.001 очень маленькое значение. Это означает, что регуляризация почти не применяется. \n",
    "Данные хорошо обусловлены, нет сильной мультиколлинеарности.\n",
    "Intercept (свободный член) - это базовое значение целевой переменной, когда все признаки равны 0. По сути это начальная стоимость страховки при \"нулевых\" условиях. Его мы не регуляризуем, потому как\n",
    "    Intercept - это просто смещение всей модели\n",
    "    Его регуляризация не имеет смысла, так как он не связан с каким-либо признаком\n",
    "    Мы хотим штрафовать только коэффициенты при признаках, чтобы бороться с переобучением\n",
    "\n",
    "Аналитическое решение Ridge регрессии\n",
    "    Формула: theta = (X^T * X + alpha * I)^(-1) * X^T * y\n",
    "    I - единичная матрица, но intercept не регуляризуем (I[0,0] = 0)\n",
    "\n",
    "Градиентный спуск с L2-регуляризацией\n",
    "    Градиент: (1/m)*X^T*(X*theta - y) + (alpha/m)*theta\n",
    "    Intercept (theta[0]) не регуляризуем\n",
    "\n",
    "Результаты: \n",
    "    Все модели показывают одинаково высокое качество - R² ≈ 0.784\n",
    "    Градиентный спуск сошелся к правильному решению - разница с аналитическим методом всего 0.0001\n",
    "    Реализация корректна - оба метода дают практически идентичные результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56043774",
   "metadata": {},
   "source": [
    "### Задание 4. Оценка обобщающей способности\n",
    "Сравнить между собой модели на тестовых данных по среднему квадрату ошибки:\n",
    "- константную - прогноз средним значением\n",
    "- из пункта 2\n",
    "- из пункта 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eebb935e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "СРАВНЕНИЕ МОДЕЛЕЙ НА ТЕСТЕ:\n",
      "Модель               MSE          R²      \n",
      "Константная          155391443.68 -0.0009 \n",
      "Линейная (аналит)    33596915.85  0.7836  \n",
      "Линейная (GD)        33608763.19  0.7835  \n",
      "Ridge (аналит)       33596923.81  0.7836  \n",
      "Ridge (GD)           33608771.05  0.7835  \n"
     ]
    }
   ],
   "source": [
    "y_pred_constant = np.full_like(y_test, np.mean(y_train))\n",
    "\n",
    "models = {\n",
    "    \"Константная\": y_pred_constant,\n",
    "    \"Линейная (аналит)\": y_pred_analytical,\n",
    "    \"Линейная (GD)\": y_pred_gd,\n",
    "    \"Ridge (аналит)\": y_pred_ridge,\n",
    "    \"Ridge (GD)\": y_pred_ridge_gd\n",
    "}\n",
    "\n",
    "print(\"СРАВНЕНИЕ МОДЕЛЕЙ НА ТЕСТЕ:\")\n",
    "print(f\"{'Модель':<20} {'MSE':<12} {'R²':<8}\")\n",
    "for name, y_pred in models.items():\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{name:<20} {mse:<12.2f} {r2:<8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546d25c",
   "metadata": {},
   "source": [
    "Обобщающая способность - способность модели хорошо работать на новых, ранее не виденных данных. \n",
    "Константная модель (прогноз средним) служит baseline для сравнения. \n",
    "MSE (Mean Squared Error) измеряет средний квадрат ошибок и является стандартной метрикой для задач регрессии.\n",
    "\n",
    "Результаты: \n",
    "    Все регрессионные модели значительно превосходят константную и показывают хорошие и почти одинаковые результаты.\n",
    "    Лучшими являются Линейная (аналитическая) и иRidge (аналитическая)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
